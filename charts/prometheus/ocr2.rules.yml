groups:
  - name: OTPE2
    interval: 15s
    rules:
      - alert: Config Digest Duplicate
        expr: max(otpe2_config_digest_duplicates_total) by(job) != 0
        labels:
          severity: critical
          team: incident-response
        annotations:
          summary: '[{{ $labels.job }}] Something really unexpected happened. Let Lorenz know.'

  - name: Telemetry Ingestion For OCR2
    interval: 15s
    rules:
      - record: sum:ocr2_telemetry_ingested_total
        expr: sum without (contract,feed_id,oracle,node_id,instance) (ocr2_telemetry_ingested_total)
      - record: bool:ocr2_telemetry_down
        expr: (rate (sum:ocr2_telemetry_ingested_total[1m])) == bool 0

      - alert: Telemetry Down (o11y)  For OCR2
        expr: bool:ocr2_telemetry_down == 1
        for: 2m
        labels:
          severity: critical
          team: monitoring
        annotations:
          summary: '[{{ $labels.job }}] OTPE is not receiving any telemetry at all.'

  - name: Contract Configuration For OCR2
    interval: 15s
    rules:
      - record: bool:ocr2_contract_oracle_active
        # use max_over_time to be resistant to exporter restarts/glitches
        expr: max(max_over_time(ocr2_contract_oracle_active[1m])) by (job,contract,feed_id,oracle,node_id) > bool 0
      - record: bool:ocr2_contract_active
        expr: sum without(oracle,node_id) (bool:ocr2_contract_oracle_active) > bool 0
      - record: bool:ocr2_oracle_active
        expr: sum without(contract,feed_id) (bool:ocr2_contract_oracle_active) > bool 0

  - name: Oracle & Feed For OCR2
    interval: 15s
    rules:
      - record: bool:ocr2_oracle_feed_telemetry_down
        expr: (sum(rate(ocr2_telemetry_ingested_total[2m])) by(job,oracle,node_id,contract,feed_id) == bool 0) * on(job,oracle,node_id,contract,feed_id) bool:ocr2_contract_oracle_active
      - record: ocr2_telemetry_message_report_req_observation_included:agg
        expr: max(ocr2_telemetry_message_report_req_observation_included) by(job,contract,feed_id,oracle,node_id)
      - record: bool:ocr2_oracle_feed_blind
        expr: (max_over_time(ocr2_telemetry_message_report_req_observation_included:agg[2m]) == bool 0) * on(job,contract,feed_id,oracle,node_id) bool:ocr2_contract_oracle_active

  - name: Oracle For OCR2
    interval: 15s
    rules:
      - record: bool:ocr2_oracle_blind
        expr: min without(contract,feed_id) (bool:ocr2_oracle_feed_blind) * bool:ocr2_oracle_active
      - record: bool:ocr2_oracle_blind_except_telemetry_down
        expr: bool:ocr2_oracle_blind * on(job) group_left() (1 - bool:ocr2_telemetry_down)

      # Oracle Blind EXCEPT Telemetry Down
      - alert: No observations from an OCR oracle
        expr: bool:ocr2_oracle_blind_except_telemetry_down == 1
        for: 2m
        labels:
          severity: critical
          team: incident-response
        annotations:
          summary: '[{{ $labels.job }}] Oracle has made no observations {{ $labels.oracle }}. Perhaps the oracle is down or having data source issues? Reach out to the node op.'

      - record: bool:ocr2_oracle_telemetry_down
        expr: min without(contract,feed_id) (bool:ocr2_oracle_feed_telemetry_down) * on(job,oracle,node_id) bool:ocr2_oracle_active
      - record: bool:ocr2_oracle_telemetry_down_except_telemetry_down
        expr: bool:ocr2_oracle_telemetry_down * on(job) group_left() (1 - bool:ocr2_telemetry_down)

      # Oracle Telemetry Down EXCEPT Telemetry Down
      - alert: No telemetry from an OCR oracle
        expr: bool:ocr2_oracle_telemetry_down_except_telemetry_down{oracle!="csa_unknown"} == 1
        for: 2m
        labels:
          severity: critical
          team: incident-response
        annotations:
          summary: '[{{ $labels.job }}] Not receiving any telemetry for {{ $labels.oracle }}. Perhaps the oracle is down or having issues with the telemetry transport? Reach out to the node op.'

  - name: Feed For OCR2
    interval: 15s
    rules:
      - record: bool:ocr2_feed_telemetry_down
        expr: min(bool:ocr2_oracle_feed_telemetry_down) by(job,contract,feed_id) * bool:ocr2_contract_active
      - record: bool:ocr2_feed_telemetry_down_except_telemetry_down
        expr: bool:ocr2_feed_telemetry_down * on(job) group_left() (1 - bool:ocr2_telemetry_down)

      # Feed Telemetry Down EXCEPT Telemetry Down
      - alert: No telemetry on an OCR feed
        expr: bool:ocr2_feed_telemetry_down_except_telemetry_down == 1
        for: 2m
        labels:
          severity: critical
          team: incident-response
        annotations:
          summary: '[{{ $labels.job }}] Not receiving any telemetry for {{ $labels.contract }}. Are all nodes down or not sending telemetry?'
      - record: bool:ocr2_contract_active:agg
        expr: max(bool:ocr2_contract_active) by(job,contract,feed_id)
      - record: bool:ocr2_feed_stalled
        expr: (sum(rate(ocr2_telemetry_feed_agreed_epoch[3m])) by(job,contract,feed_id) == bool 0) * bool:ocr2_contract_active:agg
      - record: bool:ocr2_feed_stalled_except_telemetry_down
        expr: bool:ocr2_feed_stalled * (1 - bool:ocr2_feed_telemetry_down)

      # Alert if no new round seen after 90 seconds, unless feed fails to report or OTPE is not receving any telememtry at all
      - alert: Rounds have stopped progressing on an OCR2 feed
        expr: (
          (sum(rate(ocr2_telemetry_epoch_round[3m])) by (job,contract,feed_id) < 1./90 == bool 0)
          * bool:ocr2_contract_active
          * (1-bool:ocr2_feed_telemetry_down)
          ) == 1
        labels:
          severity: critical
          team: incident-response
        annotations:
          summary: '[{{ $labels.job }}] New rounds are not being created on feed {{ $labels.contract }} at the expected rate. Maybe the feed has stalled. Reach out to node operators to corroborate this. If they are not seeing any runs, escalate and consider failing over to backup if you cannot resolve this quickly.'

      # Feed Stalled EXCEPT Feed Telemetry Down
      - alert: Epochs have stopped progressing on an OCR2 feed
        expr: bool:ocr2_feed_stalled_except_telemetry_down == 1
        labels:
          severity: critical
          team: incident-response
        annotations:
          summary: '[{{ $labels.job }}] New epochs are not being created on feed {{ $labels.contract }} at the expected rate. Maybe the feed has stalled. Reach out to node operators to corroborate this. If they are not seeing any runs, escalate and consider failing over to backup if you cannot resolve this quickly.'

      - record: ocr2_telemetry_feed_message_report_req_size:agg
        expr: max(ocr2_telemetry_feed_message_report_req_size) by(job,contract,feed_id)
      - record: ocr2_contract_config_f:agg
        expr: max(ocr2_contract_config_f) by(job,contract,feed_id)
      - record: bool:ocr2_feed_close_to_reporting_failure
        expr: (max_over_time(ocr2_telemetry_feed_message_report_req_size:agg[2m]) < bool 2*ocr2_contract_config_f:agg+1 + 2) * on(job,contract,feed_id) bool:ocr2_contract_active
      - record: bool:ocr2_feed_close_to_reporting_failure_except_feed_telemetry_down
        expr: bool:ocr2_feed_close_to_reporting_failure * (1 - bool:ocr2_feed_telemetry_down)

      # Feed Close To Reporting Failure EXCEPT Feed Telemetry Down
      - alert: OCR2 feed close to reporting failure
        expr: bool:ocr2_feed_close_to_reporting_failure_except_feed_telemetry_down == 1
        for: 2m
        labels:
          severity: critical
          team: incident-response
        annotations:
          summary: '[{{ $labels.job }}] Feed is within two oracles of reporting failure {{ $labels.contract }}. Reach out to node ops that are having issues asap and consider replacing them.'
      - record: ocr2_telemetry_feed_message_report_req_total:agg
        expr: max(ocr2_telemetry_feed_message_report_req_total) by(job,contract,feed_id)
      - record: bool:ocr2_feed_reporting_failure
        expr: (rate(ocr2_telemetry_feed_message_report_req_total:agg[4m]) == bool 0) * bool:ocr2_contract_active
      - record: bool:ocr2_feed_reporting_failure_except_feed_telemetry_down
        expr: bool:ocr2_feed_reporting_failure * (1 - bool:ocr2_feed_telemetry_down)

      # Feed Reporting Failure EXCEPT Feed Telemetry Down
      - alert: OCR2 feed reporting failure
        expr: bool:ocr2_feed_reporting_failure_except_feed_telemetry_down == 1
        labels:
          severity: critical
          team: incident-response
          name: sev1 # add this unique key/value so that this alert doesn't get grouped with the above alert
                     # this is a not great hack that's a stop gap measure For Nowâ„¢
        annotations:
          summary: '[{{ $labels.job }}] Feed is experiencing reporting failure {{ $labels.contract }}!'

  - name: Oracle & Feed Except Oracle For OCR2
    interval: 15s
    rules:
      - record: bool:ocr2_oracle_feed_blind_except_oracle_blind_except_feed_reporting_failure_except_feed_telemetry_down
        expr: (bool:ocr2_oracle_feed_blind * ignoring (contract,feed_id) group_left() (1 - bool:ocr2_oracle_blind)) * ignoring (oracle,node_id) group_left() (1 - bool:ocr2_feed_reporting_failure) * ignoring (oracle,node_id) group_left() (1 - bool:ocr2_feed_telemetry_down)

      # Oracle & Feed Blind EXCEPT Oracle Blind EXCEPT Feed Reporting Failure EXCEPT Feed Telemetry Down. Low urgency alert for async review by IR.
      - alert: Oracle not making observations on an OCR feed
        expr: bool:ocr2_oracle_feed_blind_except_oracle_blind_except_feed_reporting_failure_except_feed_telemetry_down == 1
        for: 2m
        labels:
          severity: warning
          team: incident-response
        annotations:
          summary: "[{{ $labels.job }}] Oracle {{ $labels.oracle }} is able to make observations, yet I'm not receiving any observations from it on feed {{ $labels.contract }}. Perhaps a data source issue? Reach out to the node op."

  - name: Absent metrics OTPE2
    interval: 15s
    rules:
      - record: ocr2_telemetry_ingested_total:scrapetime
        expr: timestamp(sum:ocr2_telemetry_ingested_total) # use recording rule to keep metric cardinality low

      - alert: Absent OCR telemetry ingested metric(s)
        expr: time() - max_over_time(ocr2_telemetry_ingested_total:scrapetime[1h]) > 60 # seconds
        for: 2m
        labels:
          team: monitoring
          severity: critical
        annotations:
          summary:
              '[{{ $labels.job }}] OCR2 Telemetry Ingestion has stopped for {{ $labels.job }} ({{ $labels.instance }}).
            Check the kafka prometheus instance and make sure it is healthy and pushing metrics to cortex.'

  - name: CSA Unknown
    interval: 15s
    rules:
      - alert: Unknown CSA key in OTPE2
        expr: rate(ocr2_telemetry_ingested_total{oracle="csa_unknown"}[1m]) > 0
        for: 2m
        labels:
          severity: warning
          team: monitoring
        annotations:
          summary:
            '[{{ $labels.job }}] An oracle is sending telemtry for {{ $labels.feed_id}} but is not in RDD. This should not happen and could be because of time difference between oti and otpe2 getting an update for RDD.
            Check if we keep ingesting metrics for csa_unknown, if is one time off, should be no problem. To clean the metric, restart otpe2.'
