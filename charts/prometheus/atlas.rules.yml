groups:
  - name: atlas RDS
    rules:
      - alert: RDS High CPU Utilization Warning
        expr: aws_rds_cpuutilization_average > 80
        for: 2m
        labels:
          severity: warning
          team: monitoring
        annotations:
          summary: "{{ $labels.dbinstance_identifier }} has high CPU utilization"
          description:
            '{{ $labels.dbinstance_identifier }} is over 80% CPU utilization for the past 5 minutes.'

      - alert: RDS High CPU Utilization Critical
        expr: aws_rds_cpuutilization_average > 80
        for: 2m
        labels:
          severity: critical
          team: monitoring
        annotations:
          summary: "{{ $labels.dbinstance_identifier }} has high CPU utilization"
          description:
            '{{ $labels.dbinstance_identifier }} is over 80% CPU utilization for the past 30 minutes.'
          runbook: https://github.com/smartcontractkit/alerts/blob/master/runbooks/olly/atlas.md#rds-high-cpu-utilization-critical

      - alert: RDS High CPU Utilization Sudden Increase
        expr: (rate(aws_rds_cpuutilization_average[1h:1m]) - avg_over_time(rate(aws_rds_cpuutilization_average[1h:1m])[1w:1h])) / stddev_over_time(rate(aws_rds_cpuutilization_average[1h:1m])[1w:1h]) > 5
        for: 2m
        labels:
          severity: warning
          team: monitoring
        annotations:
          summary: "{{ $labels.dbinstance_identifier }} has sudden increase in high CPU utilization"
          description:
            '{{ $labels.dbinstance_identifier }} has suddenly increased CPU utilization for the past 30 minutes. Check
            if any new queries have been deployed and ensure they are optimal (RDS Insights).'

      - alert: RDS Disk Space Warning
        expr: aws_rds_free_storage_space_average / 1000000000 < 150
        labels:
          severity: warning
          team: monitoring
        annotations:
          summary: "{{ $labels.dbinstance_identifier }} is low on disk space"
          description:
            '{{ $labels.dbinstance_identifier }} has less than 150 GB of free space left. Please bump the available
            space in the terraform module by another 100 GB: https://github.com/smartcontractkit/infra/blob/master/aws/rds/standard/prod/atlas/terragrunt.hcl#L15
            and notify o11y on slack.'

      - alert: RDS Disk Space Critical
        expr: aws_rds_free_storage_space_average / 1000000000 < 50
        labels:
          severity: critical
          team: incident-response
        annotations:
          summary: "{{ $labels.dbinstance_identifier }} is low on disk space"
          description:
            '{{ $labels.dbinstance_identifier }} has less than 50 GB of free space left. Please bump the available
            space in the terraform module by another 100 GB: https://github.com/smartcontractkit/infra/blob/master/aws/rds/standard/prod/atlas/terragrunt.hcl#L15
            and notify o11y on slack.'
          runbook: https://github.com/smartcontractkit/alerts/blob/master/runbooks/olly/atlas.md#rds-disk-space-critical

      - alert: RDS Low IO Burst Balance Critical
        expr: aws_rds_burst_balance_average < 50
        for: 2m
        labels:
          severity: critical
          team: monitoring
        annotations:
          summary: "{{ $labels.dbinstance_identifier }} is low on IO burst balance"
          description:
            '{{ $labels.dbinstance_identifier }} is low on IO burst balance. This likely means queries are performing poorly
            and eating up too many read IOPS. Query performance needs to be analyzed, or, we need to adjust the provisioned
            IOPS of the backing volume.'
          runbook: https://github.com/smartcontractkit/alerts/blob/master/runbooks/olly/atlas.md#rds-low-io-burst-balance-critical

  - name: atlas-evm exporter
    rules:
      - record: atlas_evm_exporter_db_latency:99th
        expr: histogram_quantile(0.99, rate(atlas_evm_exporter_db_latency_ms_bucket[2m]))

      - alert: Query Latency High Warning
        expr: atlas_evm_exporter_db_latency:99th > 5000 # ms, 5 seconds
        for: 2m
        labels:
          severity: warning
          team: monitoring
        annotations:
          summary: "{{ $labels.job_name }} from {{ $labels.job }} database queries are taking exceptionally long for {{ $labels.network_name }}"
          description:
            'We have a high 99th percentile query latency. (1% of queries.) This could mean our table size is
            starting to outpace index performance. This dashboard on AWS prod (https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#dashboards:name=o11y-atlas-prod-rds;start=PT72H)
            has more information. Look for a low IO Burst Balance credit and higher than normal IOPS.'

      - alert: Query Latency High Critical
        expr: atlas_evm_exporter_db_latency:99th > 10000 # ms, 10 seconds
        for: 2m
        labels:
          severity: warning
          team: monitoring
        annotations:
          summary: "{{ $labels.job_name }} from {{ $labels.job }} database queries are taking exceptionally long for {{ $labels.network_name }}"
          description:
            'We have a very high 99th percentile query latency. (1% of queries.) This could mean our table size is
            starting to outpace index performance. This dashboard on AWS prod (https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#dashboards:name=o11y-atlas-prod-rds;start=PT72H)
            has more information. Look for a low IO Burst Balance credit and higher than normal IOPS.'

      - alert: High amount of query timeouts
        expr: increase(atlas_evm_exporter_db_query_timeout[5m]) > 10
        for: 2m
        labels:
          severity: critical
          team: monitoring
        annotations:
          summary: "{{ $labels.job_name }} from {{ $labels.job }} has a high rate of query timeouts for {{ $labels.network_name }} in the past 5 minutes."
          description:
            'Check to see if there is a issue with the database connection, or if a corresponding high latency alert exists.'
          runbook: https://github.com/smartcontractkit/alerts/blob/master/runbooks/olly/atlas.md#high-amount-of-query-timeouts

      - alert: EVM block header insertion stalled (any network) - Warning
        expr: sum without(instance) (changes(atlas_evm_exporter_block_heads{network_name!="fantom-testnet"}[1m])) == 0
        for: 2m
        labels:
          severity: warning
          team: monitoring
          product: "blocks-atlas-rds-sink"
        annotations:
          summary: "{{ $labels.network_name }} has not seen new block headers"
          description:
            '{{ $labels.network_name }} has not seen new block headers for the past minute. Block headers are not being inserted
            into Postgres. Check Kafka JDBC sink connectors and atlas-rds health.'

      - alert: EVM block header insertion stalled (fantom testnet) - Warning
        expr: sum without(instance) (changes(atlas_evm_exporter_block_heads{network_name="fantom-testnet"}[1m])) == 0
        for: 2m
        labels:
          severity: warning
          team: monitoring
          product: "blocks-atlas-rds-sink"
        annotations:
          summary: "{{ $labels.network_name }} has not seen new block headers"
          description:
            '{{ $labels.network_name }} has not seen new block headers. Block headers are not being inserted
            into Postgres. Check Kafka JDBC sink connectors and atlas-rds health.'

      - alert: EVM block header insertion stalled (mainnets) - Critical
        expr: sum without(instance) (changes(atlas_evm_exporter_block_heads{network_name=~".*mainnet.*"}[1m])) == 0
        for: 2m
        labels:
          severity: critical
          team: monitoring
          product: "blocks-atlas-rds-sink"
        annotations:
          summary: "{{ $labels.network_name }} has not seen new block headers"
          description:
            '{{ $labels.network_name }} has not seen new block headers for the past 5 minutes. Block headers are not being inserted
            into Postgres. Check Kafka JDBC sink connectors and atlas-rds health.'
          runbook: https://github.com/smartcontractkit/alerts/blob/master/runbooks/olly/atlas.md#evm-block-header-insertion-stalled-mainnets---critical--evm-block-header-insertion-stalled-testnets---critical

      - alert: EVM block header insertion stalled (testnets) - Critical
        expr: sum without(instance) (changes(atlas_evm_exporter_block_heads{network_name=~".*testnet.*",network_name!~"ethereum-testnet-goerli"}[1m])) == 0
        for: 2m
        labels:
          severity: critical
          team: monitoring
          product: "blocks-atlas-rds-sink"
        annotations:
          summary: "{{ $labels.network_name }} has not seen new block headers"
          description:
            '{{ $labels.network_name }} has not seen new block headers for the past 60 minutes. Block headers are not being inserted
            into Postgres. Check Kafka JDBC sink connectors and atlas-rds health.'
          runbook: https://github.com/smartcontractkit/alerts/blob/master/runbooks/olly/atlas.md#evm-block-header-insertion-stalled-mainnets---critical--evm-block-header-insertion-stalled-testnets---critical

      - alert: EVM block header insertion stalled (ethereum-testnet-goerli) - Critical
        expr: sum without(instance) (changes(atlas_evm_exporter_block_heads{network_name="ethereum-testnet-goerli"}[1m])) == 0
        for: 2m
        labels:
          severity: critical
          team: monitoring
          product: "blocks-atlas-rds-sink"
        annotations:
          summary: "{{ $labels.network_name }} has not seen new block headers"
          description:
            '{{ $labels.network_name }} has not seen new block headers for the past 120 minutes. Block headers are not being inserted
            into Postgres. Check Kafka JDBC sink connectors and atlas-rds health.'
          runbook: https://github.com/smartcontractkit/alerts/blob/master/runbooks/olly/atlas.md#evm-block-header-insertion-stalled-mainnets---critical--evm-block-header-insertion-stalled-testnets---critical

      - alert: EVM event insertion stalled (any network) - Warning
        expr: sum without(instance) (changes(atlas_evm_exporter_event_heads[1m])) == 0
        for: 2m
        labels:
          severity: warning
          team: monitoring
          product: "events-atlas-rds-sink"
        annotations:
          summary: "{{ $labels.network_name }} has not seen new events"
          description:
            '{{ $labels.network_name }} has not seen new events for the past minute. EVM Events are not being inserted
            into Postgres. Check Kafka JDBC sink connectors and atlas-rds health.'

      - alert: EVM event insertion stalled (mainnets) - Critical
        expr: sum without(instance) (changes(atlas_evm_exporter_event_heads{network_name=~".*mainnet.*"}[1m])) == 0
        for: 2m
        labels:
          severity: critical
          team: monitoring
          product: "events-atlas-rds-sink"
        annotations:
          summary: "{{ $labels.network_name }} has not seen new events"
          description:
            '{{ $labels.network_name }} has not seen new events for the past 5 minutes. EVM Events are not being inserted
            into Postgres. Check Kafka JDBC sink connectors and atlas-rds health.'
          runbook: https://github.com/smartcontractkit/alerts/blob/master/runbooks/olly/atlas.md#evm-event-insertion-stalled-mainnets---critical--evm-event-insertion-stalled-testnets---critical

      - alert: EVM event insertion stalled (testnets) - Critical
        expr: sum without(instance) (changes(atlas_evm_exporter_event_heads{network_name=~".*testnet.*", network_name!~"fantom-testnet|ethereum-testnet-goerli"}[1m])) == 0
        for: 2m
        labels:
          severity: critical
          team: monitoring
          product: "events-atlas-rds-sink"
        annotations:
          summary: "{{ $labels.network_name }} has not seen new events"
          description:
            '{{ $labels.network_name }} has not seen new events for the past 5 minutes. EVM Events are not being inserted
            into Postgres. Check Kafka JDBC sink connectors and atlas-rds health.'
          runbook: https://github.com/smartcontractkit/alerts/blob/master/runbooks/olly/atlas.md#evm-event-insertion-stalled-mainnets---critical--evm-event-insertion-stalled-testnets---critical

      - alert: EVM event insertion stalled (fantom-testnet) - Critical
        expr: sum without(instance) (changes(atlas_evm_exporter_event_heads{network_name="fantom-testnet"}[1m])) == 0
        for: 2m
        labels:
          severity: critical
          team: monitoring
          product: "events-atlas-rds-sink"
        annotations:
          summary: "{{ $labels.network_name }} has not seen new events"
          description:
            '{{ $labels.network_name }} has not seen new events for the past 5 minutes. EVM Events are not being inserted
            into Postgres. Check Kafka JDBC sink connectors and atlas-rds health.'
          runbook: https://github.com/smartcontractkit/alerts/blob/master/runbooks/olly/atlas.md#evm-event-insertion-stalled-mainnets---critical--evm-event-insertion-stalled-testnets---critical

      - alert: EVM event insertion stalled (ethereum-testnet-goerli) - Critical
        expr: sum without(instance) (changes(atlas_evm_exporter_event_heads{network_name="ethereum-testnet-goerli"}[1m])) == 0
        for: 2m
        labels:
          severity: critical
          team: monitoring
          product: "events-atlas-rds-sink"
        annotations:
          summary: "{{ $labels.network_name }} has not seen new events"
          description:
            '{{ $labels.network_name }} has not seen new events for the past 180 minutes. EVM Events are not being inserted
            into Postgres. Check Kafka JDBC sink connectors and atlas-rds health.'
          runbook: https://github.com/smartcontractkit/alerts/blob/master/runbooks/olly/atlas.md#evm-event-insertion-stalled-mainnets---critical--evm-event-insertion-stalled-testnets---critical

      - alert: No new records on Big Query table - Critical
        expr: atlas_evm_exporter_bq_table_row_count{cluster="prometheus-mgnt-cortex-monitors-kafka",table_name!~"asset-volume|exchange-volume"} == 0
        for: 2m
        labels:
          severity: critical
          team: data-feeds
          product: "events-atlas-bq-sink"
        annotations:
          summary: "{{ $labels.table_name }} has no new records"
          description:
            '{{ $labels.table_name }} has not seen new events for the past minute.
             Check Kafka Big Query sink connectors and Big Query health.'

      - alert: Onchain monitor simplified_config insertion stalled (Solana/terra) - Warning
        expr: sum without(instance) (changes(atlas_evm_exporter_simplified_config_heads[5m])) == 0
        labels:
          severity: warning
          team: monitoring
        annotations:
          summary: "{{ $labels.network_name }} has not seen new simplified config"
          description:
            '{{ $labels.network_name }} has not seen new simplified config for the past minute. Simplified Configs are not being inserted
            into Postgres. Check Kafka JDBC sink connectors, atlas-rds health, or tom/som'

      - alert: Onchain monitor simplified_config insertion stalled (Solana/terra) - critical
        expr: sum without(instance) (changes(atlas_evm_exporter_simplified_config_heads{network_name=~".*mainnet.*"}[1m])) == 0
        for: 2m
        labels:
          severity: critical
          team: monitoring
        annotations:
          summary: "{{ $labels.network_name }} has not seen new events"
          description:
            '{{ $labels.network_name }} has not seen new simplified config for the past 90 minutes. Simplified Configs are not being inserted
           into Postgres. Check Kafka JDBC sink connectors, atlas-rds health, or tom/som'
          runbook: https://github.com/smartcontractkit/alerts/blob/master/runbooks/olly/atlas.md#onchain-monitor-simplified_config-insertion-stalled-solanaterra---critical

      - alert: Atlas topic production has stalled (mainnets) - warning
        expr: confluent_kafka_server_received_records{topic=~"[^k].*mainnet_block_headers.*|[^k].*mainnet_transactions.*|[^k].*mainnet_events.*"} == 0
        for: 2m
        labels:
          severity: warning
          team: monitoring
        annotations:
          summary: "Kafka topic {{ $labels.topic }} has not seen new messages"
          description: >
            Kafka topic {{ $labels.topic }} has not seen new messages for the past 10 minutes. Check the appropriate
            producer's logs, most likely adjust the RPC endpoint.

      - alert: Atlas topic production has stalled (mainnets) - warning
        expr: confluent_kafka_server_received_records{topic=~"[^k].*mainnet_block_headers.*|[^k].*mainnet_transactions.*|[^k].*mainnet_events.*"} == 0
        for: 2m
        labels:
          severity: warning
          team: monitoring
        annotations:
          summary: "Kafka topic {{ $labels.topic }} has not seen new messages"
          description: >
            Kafka topic {{ $labels.topic }} has not seen new messages for the past 5 minutes. Check the appropriate
            producer's logs, most likely adjust the RPC endpoint.

      - alert: Atlas topic production has stalled (testnets) - warning
        expr: confluent_kafka_server_received_records{topic=~"[^k].*testnet.*block_headers.*|[^k].*testnet.*transactions.*|[^k].*testnet.*events.*"} == 0
        for: 2m
        labels:
          severity: warning
          team: monitoring
        annotations:
          summary: "Kafka topic {{ $labels.topic }} has not seen new messages"
          description: >
            Kafka topic {{ $labels.topic }} has not seen new messages for the past 60 minutes. Check the appropriate
            producer's logs, most likely adjust the RPC endpoint.

      - alert: Atlas topic production has stalled (testnets) - warning
        expr: confluent_kafka_server_received_records{topic=~"[^k].*testnet.*block_headers.*|[^k].*testnet.*transactions.*|[^k].*testnet.*events.*"} == 0
        for: 2m
        labels:
          severity: warning
          team: monitoring
        annotations:
          summary: "Kafka topic {{ $labels.topic }} has not seen new messages"
          description: >
            Kafka topic {{ $labels.topic }} has not seen new messages for the past 30 minutes. Check the appropriate
            producer's logs, most likely adjust the RPC endpoint.

      - alert: Atlas topic consumption has stalled (mainnets) - warning
        expr: confluent_kafka_server_sent_records{topic=~"[^k].*mainnet_block_headers.*|[^k].*mainnet_transactions.*|[^k].*mainnet_events.*"} == 0
        for: 2m
        labels:
          severity: warning
          team: monitoring
        annotations:
          summary: "Kafka topic {{ $labels.topic }} has not been consumed by any consumers"
          description: >
            Kafka topic {{ $labels.topic }} has not been consumed by any consumers for the past 10 minutes. Check the various
            consumer's logs, most likely adjust the RPC endpoint.

      - alert: Atlas topic consumption has stalled (mainnets) - warning
        expr: confluent_kafka_server_sent_records{topic=~"[^k].*mainnet_block_headers.*|[^k].*mainnet_transactions.*|[^k].*mainnet_events.*"} == 0
        for: 2m
        labels:
          severity: warning
          team: monitoring
        annotations:
          summary: "Kafka topic {{ $labels.topic }} has not been consumed by any consumers"
          description: >
            Kafka topic {{ $labels.topic }} has not been consumed by any consumers for the past 5 minutes. Check the various
            consumer's logs, most likely adjust the RPC endpoint.

      - alert: Atlas topic consumption has stalled (testnets) - warning
        expr: confluent_kafka_server_sent_records{topic=~"[^k].*testnet.*block_headers.*|[^k].*testnet.*transactions.*|[^k].*testnet.*events.*"} == 0
        for: 2m
        labels:
          severity: warning
          team: monitoring
        annotations:
          summary: "Kafka topic {{ $labels.topic }} has not been consumed by any consumers"
          description: >
            Kafka topic {{ $labels.topic }} has not been consumed by any consumers for the past 60 minutes. Check the various
            consumer's logs, most likely adjust the RPC endpoint.

      - alert: Atlas topic consumption has stalled (testnets) - warning
        expr: confluent_kafka_server_sent_records{topic=~"[^k].*testnet.*block_headers.*|[^k].*testnet.*transactions.*|[^k].*testnet.*events.*"} == 0
        for: 2m
        labels:
          severity: warning
          team: monitoring
        annotations:
          summary: "Kafka topic {{ $labels.topic }} has not been consumed by any consumers"
          description: >
            Kafka topic {{ $labels.topic }} has not been consumed by any consumers for the past 30 minutes. Check the various
            consumer's logs, most likely adjust the RPC endpoint.
